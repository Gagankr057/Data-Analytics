{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Extraction in ETL Assignment"
      ],
      "metadata": {
        "id": "y0-93molVrB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : Describe different types of data sources used in ETL with suitable examples.**\n",
        "\n",
        "**Ans :** In ETL, data sources are systems from where data is extracted. Common types include:\n",
        "\n",
        "**1. Relational Databases** : Structured data stored in tables with rows and columns.\n",
        "- **Examples** : MySQL, PostgreSQL, Oracle, SQL Server\n",
        "- **Use case** : Customer records, sales transactions.\n",
        "\n",
        "**2. Flat Files** : Simple file-based data storage formats.\n",
        "- **Examples** : CSV, TXT, TSV\n",
        "- **Use case** : Daily sales reports, log files.\n",
        "\n",
        "**3. Spreadsheets** : Semi-structured data maintained by business users.\n",
        "- **Examples** : Excel (.xlsx, .xls), Google Sheets\n",
        "- **Use case** : Budget sheets, manual data entry files.\n",
        "\n",
        "**4. APIs (Web Services)** : Data accessed programmatically over the internet.\n",
        "- **Examples** : REST APIs, SOAP APIs\n",
        "- **Use case** : Weather data, stock prices, social media data.\n",
        "\n",
        "**5. Cloud Storage & SaaS Platforms** : Data hosted on cloud systems.\n",
        "- **Examples** : AWS S3, Google BigQuery, Salesforce\n",
        "- **Use case** : CRM data, analytics data."
      ],
      "metadata": {
        "id": "9b570xeKVuIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2 : What is data extraction? Explain its role in the ETL pipeline.**\n",
        "\n",
        "**Ans :** Data extraction is the process of collecting data from various source systems and bringing it into the ETL pipeline.\n",
        "\n",
        "**Role in ETL Pipeline**\n",
        "- It is the first step of ETL (Extract → Transform → Load)\n",
        "- Ensures accurate and complete data collection\n",
        "- Supports both batch and real-time data processing\n",
        "- Acts as the foundation for transformation and loading\n",
        "\n",
        "Without proper extraction, the entire ETL process can fail or produce incorrect results."
      ],
      "metadata": {
        "id": "XT1lXBEtW4eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3 : Explain the difference between CSV and Excel in terms of extraction and ETL usage.**\n",
        "\n",
        "**Ans :** CSV (Comma-Separated Values) and Excel are both commonly used data formats, but they differ significantly in how they are used in ETL processes.\n",
        "\n",
        "**CSV (Comma-Separated Values)**\n",
        "- CSV is a plain text file format.\n",
        "- It stores data in a simple row-and-column structure without formatting.\n",
        "- CSV files are lightweight and consume less memory.\n",
        "- They are faster to read and process, especially for large datasets.\n",
        "- CSV files are widely supported by almost all ETL tools.\n",
        "- Best suited for large-scale ETL pipelines.\n",
        "\n",
        "**Excel Files**\n",
        "- Excel files are binary or XML-based (.xls, .xlsx).\n",
        "- They support multiple sheets, formulas, formatting, and charts.\n",
        "- Excel files are heavier and slower to process.\n",
        "- They require additional libraries or drivers for extraction.\n",
        "- Not ideal for very large datasets.\n",
        "- Mostly used for manual analysis and reporting, not heavy ETL.\n",
        "\n",
        "CSV is preferred in ETL processes due to its simplicity, speed, and efficiency, while Excel is better suited for reporting and manual data analysis rather than large-scale ETL operations."
      ],
      "metadata": {
        "id": "lHUqstN0XUmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4 : Explain the steps involved in extracting data from a relational database.**\n",
        "\n",
        "**Ans :** Data extraction from a relational database involves systematically retrieving required data while maintaining accuracy and performance. The steps are:\n",
        "\n",
        "**1. Understand the Source Database** : Analyze database schema, tables, columns, primary keys, and relationships.\n",
        "\n",
        "**2. Establish Database Connection** : Connect to the database using JDBC/ODBC or database-specific connectors with proper credentials.\n",
        "\n",
        "**3. Select Required Data** : Identify tables and columns needed for extraction based on business requirements.\n",
        "\n",
        "**4. Write SQL Queries** : Use SELECT statements with joins, filters, and conditions to fetch relevant data.\n",
        "\n",
        "**5. Apply Filters or Incremental Logic** : Use WHERE clauses (e.g., date or ID filters) to extract only new or updated records.\n",
        "\n",
        "**6. Execute Extraction** : Run queries and retrieve data from the database.\n",
        "\n",
        "**7. Validate Extracted Data** : Check row counts, null values, and data consistency to ensure correctness.\n",
        "\n",
        "**8. Store in Staging Area** : Save extracted data temporarily for transformation and loading."
      ],
      "metadata": {
        "id": "Zu7_DE7BYCRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5 : Explain three common challenges faced during data extraction.**\n",
        "\n",
        "**Ans :** During data extraction in ETL, developers often face the following three common challenges:\n",
        "\n",
        "**1. Data Quality Issues** : Source data may contain missing values, duplicate records, incorrect formats, or inconsistent data. Poor data quality can lead to inaccurate analysis and unreliable reports.\n",
        "\n",
        "**2. Performance and Scalability Problems** : Extracting large volumes of data can slow down source systems and ETL jobs. Inefficient queries or full-table scans may impact database performance, especially in enterprise environments.\n",
        "\n",
        "**3. Schema and Source Changes** : Changes in source systems such as renamed columns, modified data types, or deleted tables can break extraction logic and cause ETL failures."
      ],
      "metadata": {
        "id": "zS2iNVnUY8r7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6 : What are APIs? Explain how APIs help in real-time data extraction.**\n",
        "\n",
        "**Ans :** APIs (Application Programming Interfaces) are mechanisms that allow different software applications to communicate with each other and exchange data in a standardized way.\n",
        "\n",
        "**Role of APIs in Real-Time Data Extraction**\n",
        "\n",
        "- APIs provide direct access to live data from source systems.\n",
        "- They enable real-time or near real-time data extraction instead of batch processing.\n",
        "\n",
        "- Data is usually returned in JSON or XML format, which is easy to process in ETL tools.\n",
        "\n",
        "- APIs support event-driven data flow, where data is fetched instantly when an event occurs.\n",
        "\n",
        "- Widely used in cloud-based, SaaS, and microservices architectures.\n",
        "\n",
        "**Example** : Fetching live weather data, stock prices, or payment transactions using REST APIs."
      ],
      "metadata": {
        "id": "lIHTglUQZj57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7 : Why are databases preferred for enterprise-level data extraction?**\n",
        "\n",
        "**Ans :** Databases are preferred for enterprise-level data extraction because they are designed to handle large, critical, and continuously growing datasets efficiently.\n",
        "\n",
        "**Reasons :**\n",
        "\n",
        "**1. High Performance & Scalability** : Databases can manage millions of records and support parallel and incremental extraction.\n",
        "\n",
        "**2. Data Consistency & Integrity** : They enforce constraints, relationships, and transactions, ensuring accurate and reliable data.\n",
        "\n",
        "**3. Security & Access Control** : Databases provide authentication, authorization, and encryption for sensitive enterprise data.\n",
        "\n",
        "**4. Efficient Querying** : SQL allows precise data selection using joins, filters, and indexes, reducing unnecessary data extraction.\n",
        "\n",
        "**Reliability & Availability** : Enterprise databases support backups, recovery, and high availability, making them dependable ETL sources."
      ],
      "metadata": {
        "id": "UquMjPeuZ-lE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8 : What steps should an ETL developer take when extracting data from large CSV files (1GB+)?**\n",
        "\n",
        "**Ans :** When working with very large CSV files, an ETL developer must ensure efficiency, performance, and data reliability. The following steps should be taken:\n",
        "\n",
        "**1.Use Chunk-Based Reading** : Read the file in smaller chunks instead of loading the entire file into memory.\n",
        "\n",
        "**2. Avoid Full Memory Load** : Stream the data line-by-line to prevent memory overflow.\n",
        "\n",
        "**3. Validate Schema Before Extraction** : Check column names, data types, and delimiters before processing.\n",
        "\n",
        "**4. Apply Filtering Early** : Remove unnecessary columns or rows during extraction to reduce data size.\n",
        "\n",
        "**5.Use Parallel Processing** : Split the file and process multiple parts simultaneously, if supported.\n",
        "\n",
        "**6. Compress and Decompress Efficiently** : Use compressed formats like .gz to reduce I/O time.\n",
        "\n",
        "**7. Implement Error Handling & Logging** : Capture rejected or corrupted records separately for review.\n",
        "\n",
        "**8. Schedule During Off-Peak Hours** : Run extraction jobs when system load is low to improve performance."
      ],
      "metadata": {
        "id": "bVm49aBlasdA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m_bO1xUVjZ2"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ]
}