{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading (ETL) Assignment"
      ],
      "metadata": {
        "id": "zZUGPxSXBEwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Data Understanding.Identify all data quality issues present in the dataset that can cause problems during data loading.**\n",
        "\n",
        "**Ans :** The following data quality issues are present in the dataset and can cause problems during data loading:\n",
        "\n",
        "**1. Duplicate records** : Order_ID O101 appears more than once, violating uniqueness expectations.\n",
        "\n",
        "**2. Missing values** : Sales_Amount contains a NULL value, which can break aggregations and constraints.\n",
        "\n",
        "**3. Invalid data type** : Sales_Amount has a text value \"Three Thousand\" instead of a numeric value.\n",
        "\n",
        "**4. Inconsistent date formats** : Dates are stored in multiple formats (DD-MM-YYYY and YYYY/MM/DD), causing parsing issues.\n",
        "\n",
        "**5. Repeated complete rows** : Identical rows with same Order_ID, Customer_ID, Sales_Amount, and Order_Date exist.\n",
        "\n",
        "**6. Schema inconsistency risk** : Column headers are shown repeatedly, indicating poor data structuring."
      ],
      "metadata": {
        "id": "j_U7Yo4hBKo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Primary Key Validation.Assume Order_ID is the Primary Key.**\n",
        "\n",
        "**a) Is the dataset violating the Primary Key**\n",
        "\n",
        "**b) Which record(s) cause this violation?**\n",
        "\n",
        "**Ans :**\n",
        "\n",
        "**a)** : Yes, the dataset violates the Primary Key rule because the primary key value must be unique and non-null, but duplicate values exist.\n",
        "\n",
        "**b)** : order with order id O101 appears twice , it breaks the primary key uniqueness."
      ],
      "metadata": {
        "id": "NKMTblkWCwZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Missing Value Analysis. Which column(s) contain missing values?**\n",
        "\n",
        "**a) List the affected records**\n",
        "\n",
        "**b) Explain why loading these records without handling missing values is risky**\n",
        "\n",
        "**Ans :** Sales_Amount column contains missing(NULL) value.\n",
        "\n",
        "**a)** Affected record is Order with **Order_id O102** , **Customer_id C002**, **Sales_Amount NULL** , **Order_Date 15-01-2024**.\n",
        "\n",
        "**b)** Loading these records without handling missing value is risky because :\n",
        "- **Incorrect aggregations** : NULL values are ignored in SUM/AVG, leading to wrong Total Sales.\n",
        "\n",
        "- **BI reporting issues** : Dashboards may show misleading KPIs.\n",
        "\n",
        "- **ETL or load failure** : If Sales_Amount is defined as NOT NULL, the load will fail.\n",
        "\n",
        "- **Data quality degradation** : Unhandled NULLs reduce trust in analytical results."
      ],
      "metadata": {
        "id": "ZmoSh205D0XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Data Type Validation.Identify records where Sales_Amount violates expected data type rules.**\n",
        "\n",
        "**a) Which record(s) will fail numeric validation?**\n",
        "\n",
        "**b) What would happen if this dataset is loaded into a SQL table with Sales_Amount as DECIMAL?**\n",
        "\n",
        "**Ans :** Sales_Amount is expected to be a numeric value.\n",
        "\n",
        "**a)** Order with **Order_ID O104** show **Sales_Amount as Three Thousand** The value \"Three Thousand\" is a string and cannot be converted to a numeric type.\n",
        "\n",
        "**b)**\n",
        "- The load will fail due to data type mismatch\n",
        "\n",
        "- ETL job may terminate or rollback\n",
        "\n",
        "- Value may be converted to NULL, causing data loss\n",
        "\n",
        "- Aggregations like SUM will produce incorrect results"
      ],
      "metadata": {
        "id": "JfBZwyyaF-GN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Date Format Consistency. The Order_column has multiple formats.**\n",
        "\n",
        "**a) List all date formats present in the dataset**\n",
        "\n",
        "**b) Why is this a problem during data loading?**\n",
        "\n",
        "**Ans :** The Order_Date column contains multiple date formats, which causes issues during data loading.\n",
        "\n",
        "**a)**\n",
        "1. DD-MM-YYYY\n",
        "\n",
        "Example: 12-01-2024, 15-01-2024, 20-01-2024, 25-01-2024\n",
        "\n",
        "2. YYYY/MM/DD\n",
        "\n",
        "Example: 2024/01/18\n",
        "\n",
        "**b)** This is a big problem during data loading because :  \n",
        "- Databases expect one consistent date format\n",
        "- ETL tools may fail to parse mixed formats\n",
        "- Some records may load as NULL or incorrect dates\n",
        "- Sorting, filtering, and time-based analysis become unreliable\n",
        "- BI reports may show wrong trends or timelines"
      ],
      "metadata": {
        "id": "OYu6r6H6IY_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Load Readiness Decision. Based on the dataset condition:**\n",
        "\n",
        "**a) Should this dataset be loaded directly into the database? (Yes/No)**\n",
        "\n",
        "**b) Justify your answer with at least three reasons**\n",
        "\n",
        "**Ans :**\n",
        "\n",
        "**a)** No, the dataset should not be loaded directly.\n",
        "\n",
        "**b)** Reasons  \n",
        "\n",
        "**1. Primary key violation** : Order_ID contains duplicate values (e.g., O101).\n",
        "\n",
        "**2. Missing values present** : Sales_Amount has NULL values which can cause incorrect aggregations.\n",
        "\n",
        "**3. Invalid data types** : Non-numeric value (\"Three Thousand\") exists in Sales_Amount.\n",
        "\n",
        "**4. Inconsistent date formats**  :Multiple date formats will cause parsing and load errors.\n"
      ],
      "metadata": {
        "id": "Pl96c3wGJ1R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Pre-Load Validation Checklist. List the exact pre-load validation checks you would perform on this dataset before loading.**\n",
        "\n",
        "**Ans :** Before loading the dataset into the database, the following pre-load validation checks should be performed:\n",
        "\n",
        "**1. Primary Key Uniqueness Check** : Ensure Order_ID values are unique.\n",
        "\n",
        "**2. NULL / Missing Value Check** : Detect NULL values in Sales_Amount.\n",
        "\n",
        "**3. Data Type Validation** : Verify Sales_Amount contains only numeric values.\n",
        "\n",
        "**4. Date Format Validation** : Standardize Order_Date to a single format.\n",
        "\n",
        "**5. Duplicate Record Detection** : Identify and remove fully duplicated rows.\n",
        "\n",
        "**6. Schema Validation**  : Confirm column names and order match the target table.\n",
        "\n",
        "**7. Range Validation** : Ensure Sales_Amount is within a valid business range.\n",
        "\n",
        "**8. Referential Integrity Check**  : Validate Customer_ID follows expected format and rules.\n",
        "\n",
        "**9. Record Count Check** : Compare source vs cleaned record count."
      ],
      "metadata": {
        "id": "TynUZDbXKu8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Cleaning Strategy. Describe the step-by-step cleaning actions required to make this dataset load-ready**\n",
        "\n",
        "**Ans :** To make the dataset load-ready, the following cleaning steps should be performed in sequence:\n",
        "\n",
        "**1. Remove / handle duplicate primary keys**\n",
        "\n",
        "- Identify duplicate Order_ID values (e.g., O101)\n",
        "- Retain one valid record or apply business rules to deduplicate\n",
        "\n",
        "**2. Handle missing values**\n",
        "\n",
        "- Replace NULL Sales_Amount with a valid value (if allowed) or reject the record\n",
        "\n",
        "**3. Fix invalid data types**\n",
        "- Convert textual values like \"Three Thousand\" to numeric or remove the record\n",
        "\n",
        "**4. Standardize date formats**\n",
        "- Convert all Order_Date values to a single format (e.g., YYYY-MM-DD)\n",
        "\n",
        "**5. Remove fully duplicate rows**\n",
        "- Eliminate identical repeated records\n",
        "\n",
        "**6. Validate schema and column consistency**\n",
        "- Ensure correct column names, order, and data types\n",
        "\n",
        "**7. Final data quality validation**\n",
        "- Recheck primary key, NULLs, data types, and date format before loading."
      ],
      "metadata": {
        "id": "smwibJPWLxlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Loading Strategy Selection.Assume this dataset represents daily sales data.**\n",
        "\n",
        "**a) Should a Full Load or Incremental Load be used?**\n",
        "\n",
        "**b) Justify your choice**\n",
        "\n",
        "**Ans :**\n",
        "\n",
        "**a)** Incremental Load should be used.\n",
        "\n",
        "**b)** justification :\n",
        "- The data is generated daily, so only new records need to be added.\n",
        "- Incremental load reduces processing time compared to full load.\n",
        "- It avoids reloading historical data, improving performance.\n",
        "- More efficient for scalable ETL pipelines and large datasets.\n",
        "- Minimizes risk of reintroducing previously cleaned data issues."
      ],
      "metadata": {
        "id": "mOBUIHK2NY4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. BI Impact Scenario**\n",
        "\n",
        "**Assume this dataset was loaded without cleaning and connected to a BI dashboard.**\n",
        "\n",
        "**a) What incorrect results might appear in Total Sales KPI?**\n",
        "\n",
        "**b) Which records specifically would cause misleading insights?**\n",
        "\n",
        "**c) Why would BI tools not detect these issues automatically?**\n",
        "\n",
        "**Ans :**\n",
        "\n",
        "**a) What incorrect results might appear in the Total Sales KPI?**\n",
        "- **Total sales may be lower than actual** : NULL Sales_Amount values are ignored in SUM calculations.\n",
        "- **Total sales may be higher than actual** : Duplicate records (e.g., duplicate O101) are counted multiple times.\n",
        "- **Inconsistent trend analysis** : Incorrect date formats affect time-based aggregation.\n",
        "\n",
        "**b) Records with misleading data.**\n",
        "- Order with Order_Id O101 hvae duplicate records.\n",
        "- Order with Order_Id O102 have missing values.\n",
        "- Order with Order_Id O104 have invalid data type.\n",
        "\n",
        "**c)Why would BI tools not detect these issues automatically?**\n",
        "\n",
        "- BI tools assume data is already cleaned\n",
        "- They do not enforce business rules or data constraints\n",
        "- BI tools focus on visualization, not data validation\n",
        "- They aggregate whatever data is provided (Garbage In â†’ Garbage Out)"
      ],
      "metadata": {
        "id": "k03JGDxVOz-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ3qdBX_A8oD"
      },
      "outputs": [],
      "source": []
    }
  ]
}